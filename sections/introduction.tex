\chapter{Introduction}

Robotic manipulation is an important problem for deploying robots that can perform everyday tasks in the real world. Human videos include numerous examples of humans interacting with objects, which can be a very useful prior for robots. After all, most common objects and tools are built with human use in mind, so observing how humans behave in the world can provide essential information that can be used towards manipulation tasks.

Another important goal in robotics is generalization in terms of environments, tasks, robot embodiments, etc... One of the reasons that other areas, such as Computer Vision and NLP, have shown strong generalization with deep learning is that large, annotated datasets can be collected relatively cheaply for vision and text. On the other hand, collecting demonstrations with robots is difficult because it is expensive, time-consuming, and requires human supervision for managing resets and safety concerns. Additionally, because robots are not widely deployed in society, datasets of large magnitude do not naturally exist on the internet.

Human videos, however, are plentiful on the internet and accessible at scale. Computer vision tools are also advanced enough to be able to reliably extract task-relevant information involving the actions of humans and objects in the scene. As a result, human videos could potentially bring us closer to the grand goal of general-purpose manipulation. Relying on human videos comes with many challenges as well, including the lack of annotated human actions and the embodiment gap between human and robot morphologies. In this thesis, we present two works that aim to overcome these challenges and enable robots to interact with objects by learning from human videos.

\vspace{0.1in}
\begin{itemize}
    \item In Chapter~\ref{cha:deft}, we investigate how human videos can be used as a prior for dexterous manipulation. Because most tools and household objects are designed to be manipulated by human hands, an anthropomorphic hand is a natural step to enable human-like interaction with common objects. Our method, \ours, explores in the real world with a safe and durable soft hand. Because learning a policy from scratch is inefficient, we develop a prior from human videos that learns to predict affordances for grasping the object. Experiments demonstrate that \ours can perform a variety of challenging tasks effectively.
    \item In Chapter~\ref{cha:reward}, we investigate how we can learn generalizable reward functions from \textit{only} human videos. While most methods that learn rewards from human videos require either (1) robot demonstrations in training or (2) goal images specified in the robot domain, we show that we can learn a multi-task reward without any in-domain information. In order to bridge the visual morphology gap, our insight is to learn a representation on data where the agents are visually removed. We show strong results on three simulation tasks. In the future, we aim to scale to more tasks.
\end{itemize}
