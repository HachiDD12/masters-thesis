\chapter{Related Work}
\label{sec:related_work}

\section{Real-world robot learning}
Real-world manipulation tasks can involve a blend of classical and learning-based methods. Classical approaches like control methods or path planning often use hand-crafted features or objectives and can often lack flexibility in unstructured settings \cite{karaman2011anytime, kuffner2000rrt, mukadam2016gaussian}. On the other hand, data-driven approaches such as deep reinforcement learning (RL) can facilitate complex behaviors in various settings, although these methods frequently rely on lots of data, privileged reward information and struggle with sample efficiency \cite{kober2008primitives, peters2010reps, lillicrap2015continuous,popov2017dataefficient, pathakICMl17curiosity}. Efforts have been made to scale end-to-end RL  \cite{levine2016learning, nair2018visual, agrawal2016learning, haarnoja2017sql, kalashnikov2018qt, kalashnikov2021mt} to the real world, but their approaches are not yet efficient enough for more complex tasks and action spaces and are reduced to mostly simple tasks even after a lot of real-world learning.  Many approaches try to improve this efficiency such as by using different action spaces \cite{vices2019martin}, goal relabeling \cite{her}, trajectory guidance \cite{levine2013guided}, visual imagined goals \cite{nair2018visual}, or curiosity-driven exploration \cite{mendonca2023alan}.

\section{Learning from Human Motion}
The field of computer vision has seen much recent success in human body and object interaction with deep neural networks.  The human hand is often parametrized with MANO, a 45-dimensional vector \cite{MANO:SIGGRAPHASIA:2017} of axes aligned with the wrist, and a 10-dimensional shape vector. MANOtorch from \cite{yang2021cpf} aligns it with the anatomical joints.   Many recent works detect MANO in monocular video \citep{wang2020rgb2hands, hmr, FrankMocap_2021_ICCV}.  Some also detect objects as well as the hand together \cite{100doh, ye2022s}.  We use FrankMocap to detect the hand for this work. 

There are many recent datasets including the CMU Mocap Database and Human3.6M \citep{ionescu2013human3} for human pose estimation, 100 Days of Hands \citep{100doh} for hand-object interactions,  FreiHand \citep{Freihand2019} for hand poses, Something-Something \citep{SomethingSomething_ICCV} for semantic interactions. ActivityNet datasets \citep{caba2015activitynet}, or YouCook~\citep{youcook} are action-driven datasets that focus on dexterous manipulation.  We use these three datasets: \citep{ego4d} is a large-scale dataset with human-object interactions, \citep{Liu_2022_CVPR} for curated human-object interactions, and \citep{EPICKITCHENS} which has many household kitchen tasks.  In addition to learning exact human motion, many others focus on learning priors from human motion. \cite{ma2022vip, nair2022r3m} learn general priors using contrastive learning on human datasets. 

\section{Learning for Dexterous Manipulation}
With recent data-driven machine learning methods, roboticists are now beginning to learn dexterous policies from human data as well.  Using the motion of a human can be directly used to control robots \citep{dexpilot, sivakumar2022robotic, qin2022from}. Moving further, human motion in internet datasets can be retargeted and used directly to pre-train robotic policies \citep{shaw2022video, mandikal2022dexvip}. Additionally, using human motion as a prior for RL can help with learning skills that are human-like \cite{rajeswaran2017learning, peng2018deepmimic, mandikal2021dexvip}. Without using human data as priors, object reorientation using RL has been recently successful in a variety of settings  \citep{andrychowicz2020learning, chen2022visual}.  Similar to established work in robot dogs which do not have an easy human analog to learn from, these methods rely on a lot of training data collected in simulation along with zero-shot transfer \citep{agarwal2023legged, margolis2022rapid}.

\section{Soft Object Manipulation}
Manipulating soft and delicate objects in a robot's environment has been a long-standing problem. Using the torque output on motors, either through measuring current or through torque sensors, is useful feedback to find out how much force a robot is applying \cite{yoshikawa1985dynamic, asada1991robot}.  Coupled with dynamics controllers, these robots can learn to not apply too much torque to the environment around them \cite{lynch2017modern, liu2017designing, khatib1987unified}.   A variety of touch sensors \cite{si2023robotsweater, yuan2017gelsight, bhirangi2021reskin, SSundaram:2019:STAG} have also been developed to  feel the environment around it and can be used as control feedback.  

Soft robotics, like our robot hand, inherently have compliant properties that make them sensitive to the environment \cite{rus2015design, wang2018toward}.  However, this introduces other difficult design challenges.  Soft materials can change properties and are difficult to manufacture.  Soft robots, including our robot hand, often do not know the end-effector tip position in a closed-loop manner \cite{butterfass2001dlr, bauer2022towards}.