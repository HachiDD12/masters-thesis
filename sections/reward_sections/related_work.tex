\section{Related Work}
\label{sec:related_work_reward}

\subsection{Reward Learning}

While many works have used Reinforcement Learning (RL) with great success \cite{silver2017alphago, Vinyals2019GrandmasterLI, rma, agarwal2023legged}, RL assumes access to a reward function which may not always be easy to define or optimize for. Inverse RL is the field for defining reward functions from demonstrations and has many prior work \cite{ziebart2008maximum, ratliff2006mmp, wulfmeier2015DeepIRL, levine2018maxent}. Many ecent works have aimed to generalize to scenarios where humans provide desired outcomes \cite{fu2018variational, singh2019end}. However, these works aim to learn rewards in single-task settings. Our work concentrates on the multi-task setting, where we hope to learn a reward function that generalizes to many tasks. In particular, we wish to learn a function that can generate rewards for many tasks given one human demonstration per task.


\subsection{Pre-training Representations for Control} 

A natural approach to address multi-tasks generalization is pre-training on large datasets. Indeed, many works  leverage large-scale data to pre-train representations for control \cite{Parisi2022TheUE, Cui2022CanFM, r3m, VIP, shah2021rrl, radasavovic22robotmaskedpretraining}. To create generalized image representations, some have trained on a wide variety of images to take advantage of the natural supervision thatâ€™s abundantly available on the internet \cite{radford21clip}. Many works have adopted these universal embeddings for downstream robotics tasks \cite{shridhar2022cliport, jang2022bc}. Others have combined language embeddings with human videos to learn representations that can be used for robot manipulation tasks \cite{shao2021concept2robot}. Similarly, \cite{r3m} applies time contrastive learning \cite{sermanet18timecontrastive}, video-language alignment \cite{nair2021lorel}, and applies a sparsity penalty to learn a robust representation from Ego4D \cite{ego4d}. This representation can in turn be utilized as a distance function for rewards.

Self-supervised representation learning from unlabeled videos encodes semantic and geometric understanding of diverse actions to define reward functions for reinforcement learning. Self-supervised representation learning from images has seen significant progress in recent years \cite{pathak2016context, chen2020simclr, caron2021dino, MAE, BYOL}, approaching the performance of fully supervised methods on ImageNet \cite{russakovsky2014imagenet}. Given the success of representation learning in the image domain, recent works adapt similar approaches for learning from video for downstream robotics applications \cite{srinivas2020curl}. \cite{radasavovic22robotmaskedpretraining} trains a single masked autoencoder \cite{MAE} on diverse, in-the-wild videos. This unified vision policy is then adapted for downstream control, significantly outperforming CLIP. \cite{sermanet18timecontrastive} proposes a time-contrastive approach learning approach from multi-view video, where different views of a robot action at the same timestamp are close in feature space, while temporally offset views should be repelled in feature space. \cite{schmeckpeper20rlvideos} directly incorporates videos collected by humans by adding videos to the replay buffer and directly performing RL on the observational data. Combining offline data with in-domain data boosts performance in the environment. \cite{zakka21xirl} addresses the generalization across agents by training representations using a cycle-consistency loss by learning from other agents demonstrating the same task that are robust to differences in shape, action, end-effector, and dynamics. 


\subsection{Robot Learning from Human Videos} 

Because collecting robot demonstrations can be costly, many works have proposed using human data, which is both plentiful and easily accessible on the internet. In recent years, many large-scale annotated human datasets have been collected, such as Something-Something \cite{SomethingSomething_ICCV}, Ego4D \cite{ego4d}, EpicKitchens \cite{epickitchens}, YouCook \cite{youcook}, and ActivityNet \cite{caba2015activitynet}. Instead of learning an intermediate representation, some works directly learn from unstructured human videos \cite{bahl22whirl, shaw2022video, bahl2023affordances}.

Other works train representations \cite{r3m, radasavovic22robotmaskedpretraining, VIP, radford21clip} that use human data. For example, VIP \cite{VIP} casts representation learning from human videos as an offline goal conditioned reinforcement learning problem to create a temporally smooth embedding for novel tasks. \cite{tian21functionaldistances} learns a visual dynamics model as well as a dynamical distance function to be used for downstream tasks. However, these approaches require a goal image with the robot at test time.

There are other methods that do not require in-domain goals by learning a classifier \cite{shao2021concept2robot, DVD}. DVD \cite{DVD} trains a discriminator to determine if two videos are performing the same task by learning from in-the-wild videos of humans. The learned feature representation can be used as a reward function for downstream robot control. However, DVD still requires in-domain robot rollouts in training to learn reward functions for downstream tasks. In our work, we build upon DVD and aim to do away with robot demonstrations at both training and test time.

