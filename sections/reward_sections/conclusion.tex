\section{Conclusion and Limitations}
\label{sec:conclusions_reward}

We presented a method building on DVD \cite{DVD} to create a reward function that is both domain-agnostic and agent-agnostic. Masking and removing the agent is an effective method to force a reward function to learn representations for tasks that involve the behavior of manipulated objects in the scene. We found that using data augmented by pretrained text-image models can further improve the robustness of the reward function at test time. Overall, our method shows promise in learning a reward function that does not rely on prior robot knowledge.

Our method also has several limitations. The core limitation of our approach is that as we train on more tasks, our model has worse performance in evaluation. Ideally, when we train on more tasks, the performance should improve slightly if not stay the same. Currently, we treat training a reward function as a binary classification problem, and this can be improved to account for similarities between similar tasks. Injecting this semantic knowledge via language embeddings would hopefully lead to a more smoother representation space that might be more effective at improving with more tasks.

Another limitation of our reward is that it is sparse. While reward functions that use the full video have additional context, it lacks the structure to evaluate partial trajectories and is sparse, unlike image-based reward functions. A reward function that evaluates partial trajectories--perhaps subvideos segmented by \textit{subgoals}--might be able to combine the best of both worlds.

Our data augmentation setup also has room for improvement. To maintain the coherency of the synthetic data generated by the model, we restricted the extent to which we augmented the frames in the video. Perhaps with more text-video generation models being open-sourced as of late, we can modify the style more aggressively while also maintaining consistency across frames. This would potentially allow for better generalization.