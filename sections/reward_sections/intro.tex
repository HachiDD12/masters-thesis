\chapter{Zero-Shot Rewards from Human Videos}
\label{cha:reward}

\section{Introduction}
\label{sec:intro_reward}

Despite recent advances in robot learning, there are many challenges in advancing towards the long-standing goal of developing generalist robotic agents. To achieve this objective, one must develop a framework that can generalize across diverse environments, tasks, and robot embodiments. Perhaps the most concise way to represent these challenges is the problem of learning a goal-conditioned reward function. Critically, we want to learn and deploy a reward function in a way that is amenable to different tasks and environments.

Traditionally, designing a robotic agent to perform tasks in the real world using reinforcement learning has proved difficult due to the sheer amount of data that needs to be collected in the process of exploring the environment. As a result, much of the advancement in reinforcement learning has come from model-free reinforcement learning in simulated environments. Whereas NLP and vision research has shown greater generalization when scaling to large datasets \cite{gpt3}, the collection and use of large, task-specific data in robot learning has encountered more challenges due to the physical cost of robot deployment. Collecting robot data at scale must provide guarantees for safety, and it requires human supervision for task resets. Many robots are trained in a single environment on a small number of tasks. 

Rather than attempting to collect robot interaction data, a growing body of work has focused on exploiting the abundance of internet data, particularly ego-centric video, inspired by the success of CLIP \cite{radford21clip}. The vast quantity of human manipulation data available online could be a useful prior for robot manipulation. In particular, the diversity of the environments and tasks depicted in internet-scale data could improve generalization when learning a goal-specific reward function. With this in mind, many recent works have collected labelled, in-the-wild, egocentric videos of humans manipulating objects and performing various tasks \cite{ego4d, smthsmth, epickitchens}.

However, there are two immediate challenges in utilizing offline human videos for learning a reward function. Firstly, there is a difference in embodiments when training with human data compared to test time where trajectories are generated in the robot's action space. Transferring rewards across this embodiment gap is necessary to learn a robot policy. Secondly, there is often a difference between human and robot trajectories in environment and visual appearance of the agents in the scene. 

What is the best way to learn a reward representation in the absence of robot data? Our approach to learn a reward function is to mask and inpaint over the agent in a visual scene to provide agent-agnostic data alignment. In visually removing the agent from the scene, we hope to learn a reward representation that focuses on relevant, environment-centric features of the scene that are modified over the course of the trajectory. With the diversity of environments in datasets like Something-Something \cite{smthsmth} and Ego4D \cite{ego4d}, we believe that we can learn generalizable reward functions across different robotic tasks in simulation environments. Additionally, with this approach we wish to use only human demonstrations in specifying a goal. In particular, due to our focus on building an agent-agnostic reward function, we investigate only the zero-shot paradigm: no robot demonstrations are used in learning a policy.

We demonstrate that our method shows promise on several tasks in a simulated environment with a Sawyer arm. More investigation is required to scale this method to a diverse range of tasks.