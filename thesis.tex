\PassOptionsToPackage{svgnames,dvipsnames}{xcolor}

\documentclass[12pt]{cmuthesis}

\usepackage[Lenny]{fncychap}
\ChNameVar{\Large}

\input{sections/packages}
\input{sections/macros}

% \draftstamp{\today}{DRAFT}

\begin{document}
\frontmatter

\pagestyle{empty}

\title{{\bf Learning from Human Videos for Robotic Manipulation}}
\author{Aditya Kannan}
\date{July 2023}
\Year{2023}
\trnumber{CMU-CS-23-124}

\committee{
\begin{tabular}{rl}
Deepak Pathak, Chair & \textit{Carnegie Mellon University} \\
Abhinav Gupta & \textit{Carnegie Mellon University} \\
\end{tabular}
}

\support{}
\disclaimer{}

\keywords{Dexterous Manipulation, Reward Learning, Learning from Videos, Deep Learning}

\maketitle

\begin{abstract}

  In recent years, many works in Computer Vision and NLP have demonstrated remarkable steps towards generalization through the collection and use of diverse datasets. However, collecting large-scale robot datasets is often difficult due to many reasons including cost, reliance on human supervision, and safety. An alternative approach is to take advantage of the accessibility and wide variety of human videos available on the internet. In this thesis, we investigate two approaches that use human videos for robotic control without relying on robot demonstrations.

  In our first work, we use human videos as a prior for dexterous manipulation. Humans are able to perform a host of skills with their hands, from making food to operating tools. In this work, we investigate these challenges, especially in the case of soft, deformable objects as well as complex, relatively long-horizon tasks. However, learning such behaviors from scratch can be data inefficient. To circumvent this, we propose a novel approach,  \ours (\textbf{DE}xterous \textbf{F}ine-\textbf{T}uning for Hand Policies), that leverages human-driven priors, which are executed directly in the real world. In order to \textit{improve} upon these priors, \ours involves an efficient online optimization procedure. With the integration of human-based learning and online fine-tuning, coupled with a soft robotic hand, \ours demonstrates success across various tasks, establishing a robust, data-efficient pathway toward general dexterous manipulation.

  In our second work, we introduce a method to learn a domain- and agent-agnostic reward function from large-scale egocentric human data. Prior approaches that use human data for reward learning either require a small sample of in-domain robot data in training or need a goal image specified in the robot's environment. In this work, we focus on the setting where only human data is available at training and test time. Our approach trains a multi-task reward function that learns to disciminate between different tasks by observing the changes in the environment.  We show that our method has strong performance on three simulation tasks \textit{without} the help of robot demonstrations in training or in-domain goals.
  
  \noindent
  The source code for this thesis document is available in open source form at:
  \begin{center}
  \url{https://github.com/adityak77/masters-thesis}
  \end{center}
\end{abstract}

% \newgeometry{left=0.5in,right=0.5in,top=1in,bottom=1.4in}
\begin{acknowledgments}
  I have been incredibly fortunate and grateful to have 
  received the opportunity to pursue the work that comprises
  my thesis. To the countless mentors, friends, and
  family who invested in my growth and encouraged me to be
  bold, I dedicate this work to you.

  First, I would like to thank Prof. Deepak Pathak, my 
  thesis  advisor, who gave me the opportunity to be 
  in an environment where I could work hard, learn, 
  and succeed. His ambition and encouragement to work
  on big ideas will always be an inspiration for me. 
  I would also like to thank Prof. Abhinav Gupta for
  serving on my thesis committee and providing feedback
  on my thesis.

  I would especially like to thank Shikhar Bahl for his
  guidance and mentorship throughout my time in the lab. 
  I am grateful for his advice, encouragement and his role
  in helping me become more independent in my research.

  I would also like to thank my collaborators, friends, 
  and everyone else who elevated my experience throughout
  my time in this lab. This includes (in alphabetical order)
  Ananye Agarwal, Ellis Brown, Lili Chen, Xuxin Cheng,
  Shivam Duggal, Zipeng Fu, Konwoo Kim, Alex Li, Edward Li, 
  Pragna Mannam, Russell Mendonca, Mihir Prabhudesai,
  Ankit Ramchandani, Aravind Sivakumar, Kenny Shaw, 
  Shagun Uppal, Haoyu Xiong, and Yufei Ye.

  I am privileged to have had countless mentors early in
  my life who set me upon the path I am today. This 
  includes my middle school teacher Jack Black, who 
  instilled in me a strong work ethic, and my high school 
  physics teacher Michael O'Byrne, who taught me to always
  keep pushing beyond my comfort zone. Furthermore, I 
  am indebted to Prof. Abraham Flaxman, Prof. Shih-Chieh
  Hsu, and the PROMYS program for taking a chance on me
  and giving me the opportunity to actively work on 
  research when I was still in high school. Continuing 
  into undergrad, I would like to thank Prof. Hosein 
  Mohimani and Mustafa Guler, whose guidance cemented my 
  desire to pursue master's research.

  I would like to thank all the family and friends who 
  have provided me with incredible support, inspiration,
  and strength. I owe a great debt to my parents for their
  love and support without which I surely would not have
  succeeded in my master's program.
\end{acknowledgments}
% \restoregeometry

\pagestyle{plain}

\tableofcontents
\addtocontents{toc}{\vspace*{-2cm}}
\listoffigures
\addtocontents{lof}{\vspace*{-2cm}}
\listoftables
\listofalgorithms

\mainmatter

\include{sections/introduction}

\include{sections/deft_sections/intro}
\include{sections/deft_sections/related_work}
\include{sections/deft_sections/background}
\include{sections/deft_sections/method}
\include{sections/deft_sections/experimental_setup}
\include{sections/deft_sections/results}
\include{sections/deft_sections/discussion}

\include{sections/reward_sections/intro}
\include{sections/reward_sections/related_work}
\include{sections/reward_sections/method}
\include{sections/reward_sections/results}
\include{sections/reward_sections/conclusion}

\chapter*{Bibliography}
\addcontentsline{toc}{chapter}{Bibliography}

\vspace{-25mm}
This bibliography contains \total{citenum} references.
\vspace{10mm}

\printbibliography[heading=none]

\end{document}

%%% Local Variables:
%%% coding: utf-8
%%% mode: latex
%%% TeX-engine: xetex
%%% End:
